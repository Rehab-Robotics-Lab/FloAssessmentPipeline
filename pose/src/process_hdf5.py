#!/usr/bin/env python3
"""Module to extract pose using openpose from hdf5 file"""

import argparse
import pathlib
import json
import numpy as np
import h5py
from pose.src.extract_depth import add_stereo_depth
from pose.src.filter import smooth_2d


def allkeys(obj):
    "Recursively find all datasets in an h5py.Group."
    keys = (obj.name,)
    if isinstance(obj, h5py.Group):
        for _, value in obj.items():
            if isinstance(value, h5py.Group):
                keys = keys + allkeys(value)
            else:
                keys = keys + (value.name,)
    return keys


def convert(directory, source, cam, rerun, algorithm):
    # pylint: disable= too-many-statements
    # pylint: disable= too-many-locals
    # pylint: disable= too-many-branches
    """Extract poses from video in hdf5 file and build new hdf
    file with poses, confidences, and other non-video data.

    Args:
        directory: The directory where 3 files can be found:
            full_data-vid.hdf: hdf5 source file with images/video to work with
                               (will not write here, only read)
            full_data-novid.hdf: the hdf5 file without image/video data
                                 (will write poses out to here)
            transforms.json: Transforms file as generated by `get_transforms`
        source: Where the file came from. Options are: 'mixed', 'podium',
                and 'robot'
        cam: The camera to process. Options are `lower` and `upper`
        rerun: Whether to rerun the pose detection if a keypoints dataset
               already exists in the hdf5_out file
        algorithm: Which pose detection algorithm to run. Options are:
                   `openpose:25B` (openpose), `openpose:135`,
                   `mp-hands` (mediapipe hands), `detectron2`
    """

    # open hdf5 file
    directory = pathlib.Path(directory)
    hdf5_in = h5py.File(directory/'full_data-vid.hdf5', 'r')
    print('opened hdf file for reading video')
    hdf5_out = h5py.File(directory/'full_data-novid.hdf5', 'r+')
    print('opened hdf file for writing poses')

    transforms = None
    try:
        with open(directory/'transforms.json', encoding='utf-8') as json_file:
            transforms = json.load(json_file)
        print('got transforms file')
    except:  # pylint: disable=bare-except
        print('cannot open transforms file, if the HDF5 files do ' +
              'not have this data, processing will fail')

    cam_root = f'/vid/{cam}'
    color_dset_name = f'{cam_root}/color/data'
    pose_dset_root = f'{cam_root}/pose/{algorithm}'

    if algorithm in ("openpose:25B", "openpose:135"):
        if algorithm in ("openpose:25B", "openpose:25Bms"):
            num_keypoints = 25
        elif algorithm == "openpose:135":
            num_keypoints = 135
        else:
            raise ValueError("invalid algorithm passed")

        preexisting_keypoints = False
        kp_dset_name = f'{pose_dset_root}/keypoints/color'
        if kp_dset_name not in hdf5_out:
            keypoints_dset = hdf5_out.create_dataset(
                kp_dset_name, (hdf5_in[color_dset_name].len(), num_keypoints, 2), dtype=np.float32)
        else:
            keypoints_dset = hdf5_out[kp_dset_name]
            preexisting_keypoints = True
            print('keypoints already exist')

        preexisting_confidence = False
        conf_dset_name = f'{pose_dset_root}/confidence'
        if conf_dset_name not in hdf5_out:
            confidence_dset = hdf5_out.create_dataset(
                conf_dset_name, (hdf5_in[color_dset_name].len(), num_keypoints), dtype=np.float32)
        else:
            confidence_dset = hdf5_out[conf_dset_name]
            preexisting_confidence = True
            print('confidences already exist')

        if (not(preexisting_keypoints and preexisting_confidence)) or rerun:
            print('running pose detections')
            # We only want to import if we are doing openpose. We could alternatively
            # put a wrapper around this whole thing. But since we aren't doing that,
            # we don't want to need imports for openpose for a different algorithm
            # pylint: disable=import-outside-toplevel
            from pose.src.openpose_wrapper import process_frames
            keypoints, params = process_frames(
                hdf5_in[color_dset_name], algorithm)
            keypoints_dset[:, :, :] = keypoints[:, :, 0:2]
            confidence_dset[:, :] = keypoints[:, :, 2]
            for key, val in params.items():
                keypoints_dset.attrs[key] = val
                confidence_dset.attrs[key] = val
        print('Adding Stereo Depth')
        smooth_2d(hdf5_out, f'{pose_dset_root}')
        add_stereo_depth(
            hdf5_in, hdf5_out, cam_root, f'{pose_dset_root}/keypoints',
            transforms[source][cam] if (source in transforms and
                                        cam in transforms[source]) else None)
        add_stereo_depth(
            hdf5_in, hdf5_out, cam_root, f'{pose_dset_root}/keypoints-median5',
            transforms[source][cam] if (source in transforms and
                                        cam in transforms[source]) else None)
        print('Done Adding Stereo Depth')
    elif algorithm == "mp-hands":
        num_frames = hdf5_in[color_dset_name].len()
        preexisting_keypoints = True
        kp_dsets = {'right': {'keypoints/color': (num_frames, 21, 3),
                              'keypoints/mp-world': (num_frames, 21, 3),
                              'confidence': (num_frames)},
                    'left': {'keypoints/color': (num_frames, 21, 3),
                             'keypoints/mp-world':  (num_frames, 21, 3),
                             'confidence': (num_frames)}}
        for hand in kp_dsets:  # pylint: disable=consider-using-dict-items
            for kp_type in kp_dsets[hand]:
                kp_dset_name = f'{pose_dset_root}/{hand}/{kp_type}'
                if kp_dset_name not in hdf5_out:
                    kp_dsets[hand][kp_type] = hdf5_out.create_dataset(
                        kp_dset_name, kp_dsets[hand][kp_type], dtype=np.float32)
                    preexisting_keypoints = False
                else:
                    kp_dsets[hand][kp_type] = hdf5_out[kp_dset_name]
                    print('keypoints already exist')
            kp_dsets[hand]['keypoints/mp-world'].attrs['desc'] = \
                'The MULTI_HAND_WORLD_LANDMARKS provided by media pipe: ' +\
                'https://google.github.io/mediapipe/solutions/hands.html#multi_hand_world_landmarks'

        if not(preexisting_keypoints) or rerun:
            print('running pose detections')
            # pylint: disable=import-outside-toplevel
            from pose.src.mphands_wrapper import process_frames
            keypoints = process_frames(hdf5_in[color_dset_name])
            for hand in kp_dsets:  # pylint: disable=consider-using-dict-items
                for kp_type in kp_dsets[hand]:
                    kp_dsets[hand][kp_type][...] = keypoints[hand][kp_type]
        print('Adding Stereo Depth')
        for hand in kp_dsets:
            smooth_2d(hdf5_out, f'{pose_dset_root}/{hand}')
            add_stereo_depth(
                hdf5_in, hdf5_out, cam_root, f'{pose_dset_root}/{hand}/keypoints',
                transforms[source][cam] if (source in transforms and
                                            cam in transforms[source]) else None)
            add_stereo_depth(
                hdf5_in, hdf5_out, cam_root, f'{pose_dset_root}/{hand}/keypoints-median5',
                transforms[source][cam] if (source in transforms and
                                            cam in transforms[source]) else None)
        print('Done Adding Stereo Depth')

    print('done processing')
    hdf5_in.close()
    hdf5_out.close()
    print('done closing')


if __name__ == '__main__':
    PARSER = argparse.ArgumentParser()

    PARSER.add_argument('-d', '--directory', type=str, required=True,
                        help='The directory to find the files to process ' +
                        'Two HDF5 files are expected: <dir>/full_data-vid.hdf5 ' +
                        'and <dir>/full_data-novid.hdf5. A file with transform ' +
                        'between the color and depth images can also be included ' +
                        'at <dir>/transforms.json')
    PARSER.add_argument("-s", "--source", choices=['podium', 'robot', 'mixed'],
                        required=True, help="Where this data was recorded")
    PARSER.add_argument("-c", "--camera", choices=['lower', 'upper'], default='lower',
                        help="Which camera to process")
    PARSER.add_argument("-o", "--overwrite", dest='rerun', action='store_true',
                        help='re-run pose detection even if keypoints ' +
                        'already exist in output. Regardless, depth extraction ' +
                        'will run')
    PARSER.add_argument("-a", "--algorithm",
                        choices=['openpose:25B', 'openpose:135', "openpose:25Bms",
                                 'mp-hands', 'detectron2'],
                        required=True,
                        help="Algorithm to run for pose estimation. " +
                        "Openpose 25B uses deep network for body only " +
                        "from whole body pose paper, openpose:25Bms uses " +
                        "the same with a multi-scale net, openpose:135 " +
                        "uses openpose with whole body (body+hand+foot+face) " +
                        "mp-hands uses the mediapipe hand pose estimator")
    ARGS = PARSER.parse_args()
    convert(ARGS.directory, ARGS.source,
            ARGS.camera, ARGS.rerun, ARGS.algorithm)
