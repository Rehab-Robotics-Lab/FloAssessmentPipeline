'''
Module to extract depth given hdf5_in and hdf5_out files
'''
import numpy as np
from tqdm import trange
from common.realsense_params import MIN_VALID_DEPTH_METERS
from common.realsense_params import MAX_VALID_DEPTH_METERS
from common.tracking_params import DEPTH_KERNEL_SIZE
from common.tracking_params import MAX_TIME_DISPARITY

assert DEPTH_KERNEL_SIZE % 2 == 1
MIN_VALID_DEPTH_MM = MIN_VALID_DEPTH_METERS*1000
MAX_VALID_DEPTH_MM = MAX_VALID_DEPTH_METERS*1000


def extract_depth(depth_img, keypoints, params):  # pylint: disable= too-many-locals
    '''
    Function to extract depth in the aligned camera frame given intrinsic and extrinsics
    '''
    # inv_kc, k_d, r_cd, t_cd = params
    inv_kc, k_d, _, _ = params
    keypoints_with_depth = np.ones(
        (keypoints.shape[0], 3))
    # Keypoints with depth appended
    keypoints_with_depth[:, :2] = keypoints[:, :2]

    # shift = (Kd @ np.asarray([[0.015],[0],[0]]))[0]

    keypoints_in_depth = (
        # x pixel pos in depth, y pixel pos in depth, ~1 this is homogenous in camera
        # k_d @ (r_cd.T @ ((inv_kc @ keypoints_with_depth.T) + t_cd))).T
        # It seems that part of using the rectified image is that the transform is alreaady done:
        k_d @ (((inv_kc @ keypoints_with_depth.T)))).T
    keypoints_with_depth = (inv_kc @ keypoints_with_depth.T).T

    for i in range(keypoints_with_depth.shape[0]):

        p_x = int(keypoints_in_depth[i, 0])
        p_y = int(keypoints_in_depth[i, 1])

        # if openpose doesn't know what it is doing, it will put points at x,y=0
        if p_x == 0 or p_y == 0:
            p_z = np.nan
        else:
            row, col = np.indices((DEPTH_KERNEL_SIZE, DEPTH_KERNEL_SIZE))
            # center the indices on p_x, p_y
            row += p_y - int(np.floor(DEPTH_KERNEL_SIZE/2))
            col += p_x - int(np.floor(DEPTH_KERNEL_SIZE/2))
            # create a mask of the indices that are valid
            valid = ((row >= 0) & (row < depth_img.shape[0]) &
                     (col >= 0) & (col < depth_img.shape[1]))
            # For some reason numpy logical_and does not short circuit.
            v_idx = np.where(valid)
            v_row = row[v_idx]
            v_col = col[v_idx]
            valid[v_idx] = ((depth_img[v_row, v_col] > MIN_VALID_DEPTH_MM) &
                            (depth_img[v_row, v_col] < MAX_VALID_DEPTH_MM))
            if np.any(valid):
                p_z = np.median(depth_img[row[valid], col[valid]])
            else:
                p_z = np.nan

        keypoints_with_depth[i] = keypoints_with_depth[i] * (p_z/1000)
    return keypoints_with_depth, keypoints_in_depth[:, :2]


def add_stereo_depth(hdf5_in, hdf5_out, cam_root, pose_dset_root, transforms=None):
    """Add stereo depth to hdf5 out file given keypoints and depth images.

    It is necessary to know the camera intrinsics and extrinsics. This can be pulled out
    by the attributes in the hdf5 files if the data was present (isn't always there).
    If not, then this can be pulled out of a transforms file which gets the transforms
    from other bag files. This transforms file is generated by `get_transforms/run`.
    This json file should be opened and this function should only be passed the sub
    dictionary indexed by the source and camera.

    This uses data from both hdf5_in and out but only writes to out.

    This requires that a previous system has already discovered and saved the best
    matching depth image to the color (done by `convert_to_hdf5/src/convert_to_hdf5.py:match_depth`)
    Any depth matches which are outside of MAX_TIME_DISPARITY will be saved as nan.

    Any depth data that falls outside of [ MIN_VALID_DEPTH_METERS, MAX_VALID_DEPTH_METERS ] will be
    saved as nan.

    Creates two new datasets a 3dkeypoints one which is in metric 3d space and a keypoints-depth
    dataset which is the keypoints found by the algorithm mapped into the depth image's image
    space. Note however that the depth image keypoints will be aligned with the color data's time.

    Args:
        hdf5_in: The opened hdf5 file with the full data with video.
        hdf5_out: The opened hdf5 file to put data into and which already has
                  keypoints extracted.
        cam_root: The name of the camera to use ('lower' or 'upper')
        transforms: The dictionary with keys that are timestamps (sec since epoch) with fields
                    'rotation': 9 element list (represents 3x3 rot array) and 'translation': 3
                    element list representing translation.
    """
    # pylint: disable= too-many-locals
    depth_match_dset_name = f'{cam_root}/color/matched_depth_index'
    color_dset_name = f'{cam_root}/color/data'
    color_time_dset_name = f'{cam_root}/color/time'
    depth_dset_name = f"{cam_root}/depth/data"
    depth_time_dset_name = f'{cam_root}/depth/time'

    keypoints_dset_name = f'{pose_dset_root}/keypoints/color'
    keypoints3d_dset_name = f'{pose_dset_root}/3dkeypoints/raw_realsense'
    keypoints_depth_dset_name = f'{pose_dset_root}/keypoints/depth'

    if keypoints3d_dset_name not in hdf5_out:
        keypoints_shape = hdf5_out[keypoints_dset_name].shape
        keypoints3d_dset = hdf5_out.create_dataset(
            keypoints3d_dset_name,
            (keypoints_shape[0], keypoints_shape[1], 3),
            dtype=np.float32)
    else:
        keypoints3d_dset = hdf5_out[keypoints3d_dset_name]
        print('You might be running the Stereo depth extraction twice')

    keypoints3d_dset.attrs['desc'] = \
        'Keypoints in 3D coordinates using the raw depth from the ' +\
        'realsense camera, indexed at keypoints. Depth is used to ' +\
        'provide x, y, and z in metric space (meters)'

    if keypoints_depth_dset_name not in hdf5_out:
        keypoints_shape = hdf5_out[keypoints_dset_name].shape
        keypoints_depth_dset = hdf5_out.create_dataset(
            keypoints_depth_dset_name,
            (keypoints_shape[0], keypoints_shape[1], 2),
            dtype=np.float32)
    else:
        keypoints_depth_dset = hdf5_out[keypoints_depth_dset_name]
        print('You might be running the Stereo depth extraction twice')

    keypoints_depth_dset.attrs['desc'] =\
        'Keypoints in the depth image frame space (pixels)'

    k_c = hdf5_in[color_dset_name].attrs['K'].reshape(3, 3)

    if ('depth_to_color-rotation' in hdf5_in[color_dset_name].attrs and
            'depth_to_color-translation' in hdf5_in[color_dset_name].attrs):
        r_cd_raw = hdf5_in[color_dset_name].attrs['depth_to_color-rotation']
        t_cd_raw = hdf5_in[color_dset_name].attrs['depth_to_color-translation']
    else:
        # transforms is already the
        time_target = hdf5_in[color_time_dset_name][0] + \
            (hdf5_in[color_time_dset_name][-1] -
             hdf5_in[color_time_dset_name][0])/2
        transform_idx = np.argmin(
            np.abs(np.asarray([float(v) for v in transforms.keys()])-time_target))
        best_transform = transforms[[*transforms][transform_idx]]
        r_cd_raw = best_transform['rotation']
        t_cd_raw = best_transform['translation']

    r_cd = np.asarray(r_cd_raw).reshape(3, 3)
    t_cd = np.asarray(t_cd_raw).reshape(3, 1)

    inv_kc = np.linalg.inv(k_c)

    k_d = hdf5_in[depth_dset_name].attrs['K'].reshape(3, 3)

    for idx in trange(hdf5_in[color_dset_name].shape[0]):
        matched_index = hdf5_in[depth_match_dset_name][idx]
        if np.abs(
                hdf5_in[depth_time_dset_name][matched_index] -
                hdf5_in[color_time_dset_name][idx]
        ) > MAX_TIME_DISPARITY:
            keypoints3d_dset[idx, :, :] = np.NaN
            keypoints_depth_dset[idx, :, :] = np.NaN
        else:
            depth_img = hdf5_in[depth_dset_name][matched_index]
            keypoints = hdf5_out[keypoints_dset_name][idx]
            kp3d, kp_depth = \
                extract_depth(depth_img, keypoints,
                              (inv_kc, k_d, r_cd, t_cd))
            invalid_indeces = np.logical_or(
                kp3d[:, 2] < MIN_VALID_DEPTH_METERS,
                kp3d[:, 2] > MAX_VALID_DEPTH_METERS
            )
            kp3d[invalid_indeces, :] = np.nan
            kp_depth[invalid_indeces, :] = np.nan
            keypoints3d_dset[idx, :, :] = kp3d
            keypoints_depth_dset[idx, :, :] = kp_depth
