'''
Module to extract depth given hdf5_in and hdf5_out files
'''
import numpy as np
from tqdm import trange
from common.realsense_params import MIN_VALID_DEPTH_METERS
from common.realsense_params import MAX_VALID_DEPTH_METERS
from common.tracking_params import DEPTH_KERNEL_SIZE
from common.tracking_params import MAX_TIME_DISPARITY

assert DEPTH_KERNEL_SIZE % 2 == 1
MIN_VALID_DEPTH_MM = MIN_VALID_DEPTH_METERS*1000
MAX_VALID_DEPTH_MM = MAX_VALID_DEPTH_METERS*1000


def extract_depth(depth_img, keypoints, params):  # pylint: disable= too-many-locals
    '''
    Function to extract depth in the aligned camera frame given intrinsic and extrinsics
    '''
    inv_kc, k_d, r_cd, t_cd = params
    keypoints_with_depth = np.ones(
        (keypoints.shape[0], keypoints.shape[1] + 1))
    keypoints_with_depth[:, :2] = keypoints  # Keypoints with depth appended

    #shift = (Kd @ np.asarray([[0.015],[0],[0]]))[0]

    keypoints_in_depth = (
        # x pixel pos in depth, y pixel pos in depth, ~1 this is homogenous in camera
        k_d @ (r_cd.T @ ((inv_kc @ keypoints_with_depth.T) - t_cd))).T
    keypoints_with_depth = (inv_kc @ keypoints_with_depth.T).T

    for i in range(keypoints_with_depth.shape[0]):

        p_x = int(keypoints_in_depth[i, 0])
        p_y = int(keypoints_in_depth[i, 1])

        # if openpose doesn't know what it is doing, it will put points at x,y=0
        if p_x == 0 or p_y == 0:
            p_z = np.nan
        else:
            row, col = np.indices((DEPTH_KERNEL_SIZE, DEPTH_KERNEL_SIZE))
            # center the indices on p_x, p_y
            row += p_y - int(np.floor(DEPTH_KERNEL_SIZE/2))
            col += p_x - int(np.floor(DEPTH_KERNEL_SIZE/2))
            # create a mask of the indices that are valid
            valid = ((row >= 0) & (row < depth_img.shape[0]) &
                     (col >= 0) & (col < depth_img.shape[1]) &
                     (depth_img[row, col] > MIN_VALID_DEPTH_MM) &
                     (depth_img[row, col] < MAX_VALID_DEPTH_MM))
            if np.any(valid):
                p_z = np.median(depth_img[row[valid], col[valid]])
            else:
                p_z = np.nan

        keypoints_with_depth[i] = keypoints_with_depth[i] * (p_z/1000)
    return keypoints_with_depth, keypoints_in_depth[:, :2]


def add_stereo_depth(hdf5_in, hdf5_out, cam_root, transforms=None):
    """Add stereo depth to hdf5 out file given keypoints and depth images.

    It is necessary to know the camera intrinsics and extrinsics. This can be pulled out
    by the attributes in the hdf5 files if the data was present (isn't always there).
    If not, then this can be pulled out of a transforms file which gets the transforms
    from other bag files. This transforms file is generated by `get_transforms/run`.
    This json file should be opened and this function should only be passed the sub
    dictionary indexed by the source and camera.

    This uses data from both hdf5_in and out but only writes to out.

    This requires that a previous system has already discovered and saved the best
    matching depth image to the color (done by `convert_to_hdf5/src/convert_to_hdf5.py:match_depth`)
    Any depth matches which are outside of MAX_TIME_DISPARITY will be saved as nan.

    Any depth data that falls outside of [ MIN_VALID_DEPTH_METERS, MAX_VALID_DEPTH_METERS ] will be
    saved as nan.


    Args:
        hdf5_in: The opened hdf5 file with the full data with video.
        hdf5_out: The opened hdf5 file to put data into and which already has
                  keypoints extracted.
        cam_root: The name of the camera to use ('lower' or 'upper')
        transforms: The dictionary with keys that are timestamps (sec since epoch) with fields
                    'rotation': 9 element list (represents 3x3 rot array) and 'translation': 3
                    element list representing translation.
    """
    # pylint: disable= too-many-locals
    color_dset_name = f'{cam_root}/color/data'
    depth_match_dset_name = f'{cam_root}/color/matched_depth_index'
    color_time_dset_name = f'{cam_root}/color/time'
    depth_dset_name = f'{cam_root}/depth/data'
    depth_time_dset_name = f'{cam_root}/depth/time'
    keypoints2d_dset_name = f'{cam_root}/openpose/keypoints'
    keypoints3d_dset_name = f'{cam_root}/openpose/keypoints-3d'
    keypoints_in_depth_dset_name = f'{cam_root}/openpose/keypoints-depth'
    keypoints3d_dset = None
    keypoints_in_depth_dset = None

    if keypoints3d_dset_name not in hdf5_out:
        keypoints3d_dset = hdf5_out.create_dataset(
            keypoints3d_dset_name, (hdf5_in[color_dset_name].len(), 25, 3), dtype=np.float32)
    else:
        keypoints3d_dset = hdf5_out[keypoints3d_dset_name]
        print('You might be running the Stereo depth extraction twice')
    if keypoints_in_depth_dset_name not in hdf5_out:
        keypoints_in_depth_dset = hdf5_out.create_dataset(
            keypoints_in_depth_dset_name, (hdf5_in[color_dset_name].len(), 25, 2), dtype=np.float32)
    else:
        keypoints_in_depth_dset = hdf5_out[keypoints_in_depth_dset_name]

    k_c = hdf5_in[color_dset_name].attrs['K'].reshape(3, 3)

    if ('depth_to_color-rotation' in hdf5_in[color_dset_name].attrs and
            'depth_to_color-translation' in hdf5_in[color_dset_name].attrs):
        r_cd_raw = hdf5_in[color_dset_name].attrs['depth_to_color-rotation']
        t_cd_raw = hdf5_in[color_dset_name].attrs['depth_to_color-translation']
    else:
        # transforms is already the
        time_target = hdf5_in[color_time_dset_name][0] + \
            (hdf5_in[color_time_dset_name][-1] -
             hdf5_in[color_time_dset_name][0])/2
        transform_idx = np.argmin(
            np.abs(np.asarray([float(v) for v in transforms.keys()])-time_target))
        best_transform = transforms[[*transforms][transform_idx]]
        r_cd_raw = best_transform['rotation']
        t_cd_raw = best_transform['translation']

    r_cd = np.asarray(r_cd_raw).reshape(3, 3)
    t_cd = np.asarray(t_cd_raw).reshape(3, 1)

    inv_kc = np.linalg.inv(k_c)

    k_d = hdf5_in[depth_dset_name].attrs['K'].reshape(3, 3)

    for idx in trange(hdf5_in[color_dset_name].shape[0]):
        matched_index = hdf5_in[depth_match_dset_name][idx]
        if np.abs(
                hdf5_in[depth_time_dset_name][matched_index] -
                hdf5_in[color_time_dset_name][idx]
        ) > MAX_TIME_DISPARITY:
            keypoints3d_dset[idx, :, :] = np.NaN
            keypoints_in_depth_dset[idx, :, :] = np.NaN
        else:
            depth_img = hdf5_in[depth_dset_name][matched_index]
            keypoints = hdf5_out[keypoints2d_dset_name][idx]
            kp3d, kp_depth = \
                extract_depth(depth_img, keypoints,
                              (inv_kc, k_d, r_cd, t_cd))
            invalid_indeces = np.logical_or(
                kp3d[:, 2] < MIN_VALID_DEPTH_METERS,
                kp3d[:, 2] > MAX_VALID_DEPTH_METERS
            )
            kp3d[invalid_indeces, :] = np.nan
            kp_depth[invalid_indeces, :] = np.nan
            keypoints3d_dset[idx, :, :] = kp3d
            keypoints_in_depth_dset[idx, :, :] = kp_depth
